{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# from model.GPT2ModelWithPreFixTuning import GPT2LMHeadMoelWithPrefixTuning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--data_root', type=str, default='/home/haotian/Molecule_Generation/MG/ECloudGen_new/mol_data/ecloud')\n",
    "    parser.add_argument('--checkpoint', type=str, default='./ckpt')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output')\n",
    "    # Training Configuration\n",
    "    parser.add_argument('--gpus', type=int, default=1) # Specify either a list of GPU devices or an integer (0 for no GPU).\n",
    "    parser.add_argument('--num_workers', type=int, default=0) # Number of dataloader worker processes.\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--max_epochs', type=int, default=10)\n",
    "    parser.add_argument('--max_length', type=int, default=256)\n",
    "\n",
    "    # Optimizer args\n",
    "    parser.add_argument('--learning_rate', type=float, default=5e-4)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "    parser.add_argument('--adam_eps', type=float, default=1e-8)\n",
    "    parser.add_argument('--adam_betas', type=tuple, default=(0.9, 0.999))\n",
    "    parser.add_argument('--scheduler_T_max', type=int, default=1_000)\n",
    "    parser.add_argument('--final_learning_rate', type=float, default=5e-8)\n",
    "\n",
    "    parser.add_argument('--vocab_size', type=int, default=1072)\n",
    "\n",
    "    # Model configuration\n",
    "    # parser.add_argument('--n_layer', type=int, default=12)\n",
    "    # parser.add_argument('--n_head', type=int, default=12)\n",
    "    # parser.add_argument('--n_embd', type=int, default=768)\n",
    "    parser.add_argument('--n_layer', type=int, default=8)\n",
    "    parser.add_argument('--n_head', type=int, default=8)\n",
    "    parser.add_argument('--n_embd', type=int, default=768)\n",
    "\n",
    "    # Prefix Tuning Configuration\n",
    "    parser.add_argument('--prefix_length', type=int, default=128)\n",
    "    parser.add_argument('--prefix_dropout', type=float, default=0.1) # modified from 0.1\n",
    "    args = parser.parse_args([])\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 104/854 [00:00<00:01, 518.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 211/854 [00:00<00:01, 522.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 264/854 [00:00<00:01, 497.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 369/854 [00:00<00:00, 511.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 477/854 [00:00<00:00, 524.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 585/854 [00:01<00:00, 530.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 693/854 [00:01<00:00, 532.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 801/854 [00:01<00:00, 530.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 854/854 [00:01<00:00, 524.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "Creation Done.\n"
     ]
    }
   ],
   "source": [
    "create_h5('./mol_data/ecloud.h5', './mol_data/ecloud', './mol_data/moses2.csv', tokenizer_filename='./vocab/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "class DecipherDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Data contains the ecloud, conditions, and following smiles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_root:str, tokenizer, max_length=64, csv_file='./mol_data/moses2.csv', tokenizer_filename='./vocab/tokenizer.json') -> None:\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length - 1 # save space for prefix tokens\n",
    "        self.csv_file = csv_file\n",
    "        self.tokenizer_filename = tokenizer_filename\n",
    "\n",
    "        if data_root.endswith('.h5'):\n",
    "            self.h5_path = data_root\n",
    "            self.data_root = data_root[:-3]\n",
    "        else:\n",
    "            self.h5_path = data_root + '.h5'\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        f = h5py.File(self.h5_path, 'r')\n",
    "\n",
    "        return len(f['eclouds'])\n",
    "    \n",
    "    def __getitem__(self, i: int):\n",
    "\n",
    "        if not os.path.exists(self.h5_path):\n",
    "            self.create_h5()\n",
    "\n",
    "        f = h5py.File(self.h5_path, 'r')\n",
    "        ecloud_item = f['eclouds'][i]\n",
    "        condition_item = f['conditions'][i]\n",
    "        smiles_item = f['smiles'][i]\n",
    "        f.close()\n",
    "        return smiles_item, ecloud_item, condition_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {}\n",
    "batch['input_ids']= torch.tensor(DecipherDataset('./data/ecloud', tokenizer=4)[0][0]).unsqueeze(0)\n",
    "batch['prefix_tokens_ecloud'] = torch.tensor(DecipherDataset('./data/ecloud', tokenizer=4)[0][1]).unsqueeze(0)\n",
    "batch['prefix_tokens_condition'] = torch.tensor(DecipherDataset('./data/ecloud', tokenizer=4)[0][2]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 135,  80, 726,  65, 295,  28, 667,  39, 149, 143,   2,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecipherDataset('./data/ecloud', tokenizer=4)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecipherDataset('./data/ecloud', tokenizer=4)[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7417, -0.837 , 93.56  ], dtype=float16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecipherDataset('./data/ecloud', tokenizer=4)[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([1, 32, 32, 32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape, batch['prefix_tokens_ecloud'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 135,  80, 726,  65, 295,  28, 667,  39, 149, 143,   2,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.prefix_encoder import GPT2PrefixEncoder\n",
    "\n",
    "prefix_encoder = GPT2PrefixEncoder(\n",
    "            num_layers=args.n_layer, \n",
    "            num_heads=args.n_head, \n",
    "            input_size=args.n_embd,\n",
    "            prefix_len_ecloud=args.prefix_length,\n",
    "            prefix_dropout=args.prefix_dropout,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "prefix_encoder = prefix_encoder.to(device)\n",
    "\n",
    "batch['prefix_tokens_ecloud'] = batch['prefix_tokens_ecloud'].to(device).float()\n",
    "batch['prefix_tokens_condition'] = batch['prefix_tokens_condition'].to(device).float()\n",
    "batch['input_ids'] = batch['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = prefix_encoder(batch['prefix_tokens_ecloud'], batch['prefix_tokens_condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['prefix_tokens_ecloud'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = prefix_encoder.subencoder(batch['prefix_tokens_ecloud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.transpose(0, 1).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8, 133, 96])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8, 133, 96])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    # Count total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Convert total parameters to MB and GB (assuming float32 for each parameter)\n",
    "    total_size_MB = total_params * 4 / (1024 ** 2)  # 4 bytes for each float32\n",
    "    total_size_GB = total_params * 4 / (1024 ** 3)  # 4 bytes for each float32\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Total size of parameters in MB: {total_size_MB}\")\n",
    "    print(f\"Total size of parameters in GB: {total_size_GB}\")\n",
    "    return total_params, total_size_MB, total_size_GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.common import FFN\n",
    "ecloud = batch['prefix_tokens_ecloud'].float()\n",
    "condition = batch['prefix_tokens_condition'].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['prefix_tokens_ecloud'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = prefix_encoder.subencoder(ecloud) # (sl, bz, d_model)   sequence_length, batch_size, model_demension\n",
    "input_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = input_tokens.transpose(0, 1).transpose(1, 2)  # torch.Size([1, 768, 64])    (bz, d_model_sl)\n",
    "input_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 128])\n"
     ]
    }
   ],
   "source": [
    "encoded_sl = 64\n",
    "len_trans = FFN(encoded_sl, args.prefix_length, args.prefix_length)\n",
    "input_tokens = len_trans(input_tokens)\n",
    "print(input_tokens.shape)\n",
    "input_tokens = input_tokens.transpose(1, 2)\n",
    "past_key_values_ecloud = prefix_encoder.control_trans(input_tokens) # ( bz,  prefix_len, 2 * num_layers * num_heads * n_embd_per_head)\n",
    "                                                                    #   1,   128,        2*8*8*(768/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values_conditions = prefix_encoder.condition_trans(condition)\n",
    "                            # (batch_size, prefix_len_condition * 2*prefix_len_condition * numder_layers * num_embd_per_head )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_batch_size = ecloud.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice No.1: Each layer has its own prefix\n",
    "past_key_values_ecloud = past_key_values_ecloud.view(curr_batch_size, \n",
    "                                                     prefix_encoder.prefix_len_ecloud , \n",
    "                                                     2 * prefix_encoder.num_layers, \n",
    "                                                     prefix_encoder.num_heads, \n",
    "                                                     prefix_encoder.n_embd_per_head)   # torch.Size([1, 128, 16, 8, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 16, 8, 96])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values_ecloud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values_conditions = past_key_values_conditions.view(curr_batch_size, \n",
    "                                                             prefix_encoder.prefix_len_condition, \n",
    "                                                             2 * prefix_encoder.num_layers, \n",
    "                                                             prefix_encoder.num_heads, \n",
    "                                                             prefix_encoder.n_embd_per_head)  # torch.Size([1, 5, 16, 8, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16, 8, 96])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values_conditions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat two prefix\n",
    "past_key_values = torch.cat([past_key_values_ecloud, past_key_values_conditions], dim=1) \n",
    "# (bz, prefix_len, 2 * num_layers, num_heads, n_embd_per_head)\n",
    "# torch.Size([1, 133, 16, 8, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = past_key_values.permute(2, 0, 3, 1, 4).split(2) # 2*num_layers, bs, num_heads, prefix_len, n_embd_per_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8, 133, 96])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8, 133, 96])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2PreTrainedModel, GPT2Model, GPT2Config\n",
    "config = GPT2Config(\n",
    "    prefix_length=args.prefix_length,  \n",
    "    prefix_dropout=args.prefix_dropout,\n",
    "    batch_size=args.batch_size,\n",
    "    vocab_size=args.vocab_size,\n",
    "    n_layer=args.n_layer,\n",
    "    n_head=args.n_head,\n",
    "    n_embd=args.n_embd,\n",
    "    n_positions = args.max_length,\n",
    "    n_ctx = args.max_length,\n",
    "    )\n",
    "transformer = GPT2Model(config)\n",
    "transformer = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(1,0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch['input_ids'].int()\n",
    "attention_mask = None\n",
    "token_type_ids = None\n",
    "position_ids = None\n",
    "head_mask = None\n",
    "inputs_embeds = None\n",
    "encoder_hidden_states = None\n",
    "encoder_attention_mask = None\n",
    "use_cache = None\n",
    "output_attentions = None\n",
    "output_hidden_states = None\n",
    "return_dict = True\n",
    "\n",
    "transformer_outputs = transformer(\n",
    "    input_ids,\n",
    "    past_key_values=past_key_values,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    "    position_ids=position_ids,\n",
    "    head_mask=head_mask,\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    "    use_cache=use_cache,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    "    return_dict=return_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 768])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_outputs[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecloud2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
